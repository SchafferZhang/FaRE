import os
import numpy as np
from tqdm import tqdm
import logging

from multiprocessing.pool import Pool

from sklearn.metrics.pairwise import paired_distances as compute_paired_distances
from sklearn.metrics import pairwise_distances as compute_pairwise_distances

import mxnet as mx
from mxnet.gluon.data.dataloader import DataLoader
from mxnet.gluon.utils import split_and_load

from ..io import File, Signature, Template, save_pkl
from ..metrics import BiometricCompareProtocol, BiometricCloseSearchProtocol, BiometricOpenSearchProtocol
from ..metrics.metric_funcs import compute_pairwise_distances_mask, compute_paired_distances_mask
from ..utils.dataloader import ImagePathDataset, ImagePathLabelDataset, PairedImagePathLabelDataset
from ..utils.template_generator import compute_mean


RegisterTemplateTransform = {'mean': compute_mean}


class IMDB(object):
    """
    Abstract dataset class, all datasets should have this interface
    """
    def parse_data_set(self):
        """
        Parse the dataset
        :return:
        """
        raise NotImplementedError

    def load_and_compute_similarities(self, **kwargs):
        """
        Compute the similarity from the features file saved on disk
        :return:
        """
        raise NotImplementedError

    def generate_and_compute_similarities(self, **kwargs):
        """
        Compute the similarity from the features generated by forwarding the DNN
        :return:
        """
        raise NotImplementedError

    def __init__(self, db_dir=None, im_ext='.jpg', num_folds=10, ft_dir=None, ft_ext='.txt',
                 use_mask=False, mask_ext='.occ', fmode='txt', flatten=True, template_transform=None, save_dir=None):
        """
        Superclass Dataset
        :param db_dir: dataset image directory
        :param im_ext: image extension
        :param num_folds: number of folds
        :param ft_dir: features directory
        :param ft_ext: feature extension
        :param use_mask: use feature mask
        :param mask_ext: mask extension
        :param fmode: file read and write mode
        :param flatten: flatten the features
        :param template_transform: template transformation/generator
        :param save_dir: save directory
        """
        self.db_dir = db_dir
        self.im_ext = im_ext

        self.ft_dir = ft_dir
        self.ft_ext = ft_ext
        self.use_mask = use_mask
        self.mask_ext = mask_ext

        self.num_folds = num_folds
        self.num_iters = 1

        self.fmode = fmode
        self.flatten = flatten

        self.template_tforms = template_transform

        self.save_dir = save_dir

        self.cur_dir = os.path.dirname(__file__)

        self.protocols = []

    def save_protocols(self, file_name):
        """
        Save protocol to file
        :param file_name: file name, the total path will be join(self.save_dir, file_name)
        :return:
        """
        if self.save_dir is not None and not os.path.exists(self.save_dir):
            os.makedirs(self.save_dir)

        file_path = os.path.join(self.save_dir, file_name) if self.save_dir is not None else file_name

        if os.path.exists(file_path):
            logging.warning('%s exists. Skip saving')
        else:
            save_pkl(file_path, self.protocols)


def load_paired_features_and_compute_similarity(lst):
    """list sequence is [feature 1 path, feature 2 path, label, metrics, file mode, flatten mode]"""
    f1_path, f2_path, lb, metric, fmode, flatten = lst
    try:
        f1 = Signature(f1_path, fmode=fmode, flatten=flatten).features
        f2 = Signature(f2_path, fmode=fmode, flatten=flatten).features

        if len(f1.shape) == 1:
            f1 = np.reshape(f1, (1, -1))

        if len(f2.shape) == 1:
            f2 = np.reshape(f2, (1, -1))

        sim = 1 - compute_paired_distances(f1, f2, metric=metric)[0]
    except Exception as e:
        logging.warning(str(e))
        sim = 0 if lb else 1

    return sim, lb


def load_paired_features_and_compute_similarity_mask(lst):
    """list sequence is
    [feature 1 path, feature 2 path, occ 1 path, occ 2 path, label, metrics, file mode, flatten mode]"""
    f1_path, f2_path, o1_path, o2_path, lb, metric, fmode, flatten = lst
    try:
        s1 = Signature(f1_path, o1_path, fmode=fmode, flatten=flatten)
        s2 = Signature(f2_path, o2_path, fmode=fmode, flatten=flatten)
        sim = 1 - compute_paired_distances_mask(s1.features, s2.features, s1.occ, s2.occ, metric=metric)[0]
    except Exception as e:
        logging.warning(str(e))
        sim = 0 if lb else 1

    return sim, lb


class VerificationDataset(IMDB):
    """Verification dataset"""
    def __init__(self, **kwargs):
        super(VerificationDataset, self).__init__(**kwargs)
        self.folds = [{'list_a': [], 'list_b': [], 'labels': []} for _ in range(self.num_folds)]

    def parse_data_set(self):
        raise NotImplementedError

    def load_and_compute_similarities(self, metric='cosine', template_transform=None, **kwargs):

        for i, fold in enumerate(self.folds):
            total = len(fold['labels'])
            ft_path1, ft_path2, occ_path1, occ_path2, labels = [], [], [], [], []

            if isinstance(fold['list_a'][0], File):
                for f1, f2, lb in zip(fold['list_a'], fold['list_b'], fold['labels']):
                    f1_path = os.path.join(self.ft_dir, f1.im_path.replace(self.im_ext, self.ft_ext))
                    f2_path = os.path.join(self.ft_dir, f2.im_path.replace(self.im_ext, self.ft_ext))

                    ft_path1.append(f1_path)
                    ft_path2.append(f2_path)
                    labels.append(lb)

                    if self.use_mask:
                        o1_path = f1_path.replace(self.ft_ext, self.mask_ext)
                        o2_path = f2_path.replace(self.ft_ext, self.mask_ext)

                        occ_path1.append(o1_path)
                        occ_path2.append(o2_path)

                if self.use_mask:
                    # multi-processing to load the features and compute the similarity
                    with Pool() as p:
                        content = list(tqdm(p.imap(load_paired_features_and_compute_similarity_mask,
                                                   zip(ft_path1, ft_path2, occ_path1, occ_path2, labels,
                                                       [metric] * total,
                                                       [self.fmode] * total, [self.flatten] * total)),
                                            total=total))
                        content = np.array(content)
                else:
                    # multi-processing to load the features and compute the similarity
                    with Pool() as p:
                        content = list(tqdm(p.imap(load_paired_features_and_compute_similarity,
                                                   zip(ft_path1, ft_path2, labels, [metric] * total,
                                                       [self.fmode] * total,
                                                       [self.flatten] * total)),
                                            total=total))
                        content = np.array(content)

                similarity, labels = content[:, 0], content[:, 1]

            elif isinstance(fold['list_a'][0], Template):
                if isinstance(fold['list_a'][0].signature, str):
                    # signature is signature path
                    for t1, t2, lb in zip(fold['list_a'], fold['list_b'], fold['labels']):
                        # load the signature
                        ft_path1.append(t1.signature)
                        ft_path2.append(t2.signature)
                        labels.append(lb)

                    with Pool() as p:
                        content = list(tqdm(p.imap(load_paired_features_and_compute_similarity,
                                                   zip(ft_path1, ft_path2, labels, [metric] * total,
                                                       [self.fmode] * total,
                                                       [self.flatten] * total)),
                                            total=total))
                        content = np.array(content)

                    similarity, labels = content[:, 0], content[:, 1]

                else:
                    # type(fold['list_a'][0].signature) == Signature:
                    # load the signature
                    ft1_lst, ft2_lst, labels = [], [], []
                    for t1, t2, lb in zip(fold['list_a'], fold['list_b'], fold['labels']):
                        ft1_lst.append(t1.signature.features)
                        ft2_lst.append(t2.signature.features)

                    ft1_lst, ft2_lst = np.array(ft1_lst), np.array(ft2_lst)

                    similarity = 1 - compute_paired_distances(ft1_lst, ft2_lst, metric=metric)
                    labels = np.array(labels)
            else:
                raise NotImplementedError

            self.protocols.append(BiometricCompareProtocol(labels, similarity))

    def generate_and_compute_similarities(self, inference, data_transform, bs, ctx=(mx.gpu(0)), norm_embeds=False,
                                          metric='cosine', output_transform=None, template_transform=None,
                                          save_template=False, **kwargs):

        os.environ['MXNET_CUDNN_AUTOTUNE_DEFAULT'] = '0'
        if isinstance(ctx, tuple):
            ctx = list(ctx)

        # IMPLEMENT TEMPLATE DATA LOADER
        p_bar = tqdm(total=self.num_iters)

        template_transform = 'mean' if template_transform is None else template_transform

        if isinstance(template_transform, str):
            template_transform = RegisterTemplateTransform[template_transform]

        for i, fold in enumerate(self.folds):
            if isinstance(fold['list_a'][0], File):
                pairs = [[f1.im_path, f2.im_path, lb] for f1, f2, lb in zip(fold['list_a'], fold['list_b'], fold['labels'])]
                total = len(pairs)
                dataset = PairedImagePathLabelDataset(self.db_dir, pairs, transform=data_transform)
                bs = min(bs, total)
                dataloader = DataLoader(dataset, bs, last_batch='keep', pin_memory=True)

                similarity, labels = [], []
                for im1_batch, im2_batch, lb_batch in dataloader:
                    im1_lst = split_and_load(im1_batch, ctx)
                    im2_lst = split_and_load(im2_batch, ctx)
                    lb_lst = split_and_load(lb_batch, ctx)

                    for im1, im2, lb in zip(im1_lst, im2_lst, lb_lst):
                        f1 = inference(im1)
                        f2 = inference(im2)

                        if (isinstance(f1, tuple) or isinstance(f1, list)) and (isinstance(f2, tuple) or isinstance(f2, list)):
                            f1 = output_transform(f1)
                            f2 = output_transform(f2)

                        if norm_embeds:
                            f1 = mx.nd.L2Normalization(f1, mode='instance')
                            f2 = mx.nd.L2Normalization(f2, mode='instance')

                        f1 = f1.asnumpy()
                        f2 = f2.asnumpy()
                        sim = 1 - compute_paired_distances(f1, f2, metric=metric)

                        similarity.append(sim)
                        labels.append(lb.asnumpy())

                        p_bar.update(lb.shape[0])

                similarity, labels = np.concatenate(similarity), np.concatenate(labels)

            elif isinstance(fold['list_a'][0], Template):
                similarity, labels = [], []

                for t1, t2, lb in zip(fold['list_a'], fold['list_b'], fold['labels']):
                    if save_template:
                        if os.path.exists(os.path.join(self.save_dir, str(t1.template_id) + self.ft_ext)) and \
                        os.path.exists(os.path.join(self.save_dir, str(t2.template_id) + self.ft_ext)):
                            continue

                    im_paths = [im_path for im_path in t1.im_paths if os.path.exists(os.path.join(self.db_dir, im_path))]
                    dataset = ImagePathDataset(self.db_dir, im_paths, transform=data_transform)
                    dataloader = DataLoader(dataset, min(bs, len(im_paths)), last_batch='keep', pin_memory=False)

                    template_embeds = []
                    for im_batch in dataloader:
                        im_lst = split_and_load(im_batch, ctx)

                        for im in im_lst:
                            embeds = inference(im)

                            if isinstance(embeds, tuple) or isinstance(embeds, list):
                                embeds = output_transform(embeds)

                            if self.flatten:
                                embeds = mx.nd.flatten(embeds)

                            if norm_embeds:
                                embeds = mx.nd.L2Normalization(embeds, mode='instance')

                            embeds = embeds.asnumpy()

                            template_embeds.append(embeds)

                    f1 = template_transform(template_embeds)

                    im_paths = [im_path for im_path in t2.im_paths if
                                os.path.exists(os.path.join(self.db_dir, im_path))]
                    dataset = ImagePathDataset(self.db_dir, im_paths, transform=data_transform)
                    dataloader = DataLoader(dataset, min(bs, len(im_paths)), last_batch='keep', pin_memory=False)

                    template_embeds = []
                    for im_batch in dataloader:
                        im_lst = split_and_load(im_batch, ctx)

                        for im in im_lst:
                            embeds = inference(im)

                            if isinstance(embeds, tuple) or isinstance(embeds, list):
                                embeds = output_transform(embeds)

                            if self.flatten:
                                embeds = mx.nd.flatten(embeds)

                            if norm_embeds:
                                embeds = mx.nd.L2Normalization(embeds, mode='instance')

                            embeds = embeds.asnumpy()

                            template_embeds.append(embeds)

                    f2 = template_transform(template_embeds)

                    sim = 1 - compute_paired_distances(f1, f2, metric=metric)

                    similarity.append(sim)
                    labels.append(lb)

                    if save_template:
                        assert t1.template_id is not None, 'Template 1 ID cannot be None'
                        t1.load_signature(template_embeds)
                        t1.save_signature(os.path.join(self.save_dir, str(t1.template_id) + self.ft_ext))

                        assert t2.template_id is not None, 'Template 2 ID cannot be None'
                        t2.load_signature(template_embeds)
                        t2.save_signature(os.path.join(self.save_dir, str(t2.template_id) + self.ft_ext))

                similarity, labels = np.concatenate(similarity), np.concatenate(labels)
            else:
                raise NotImplementedError

            self.protocols.append(BiometricCompareProtocol(labels, similarity))

        p_bar.close()

    def compute_metric(self, metric='ROC'):
        for i in range(len(self.protocols)):
            if metric.upper() == 'ROC':
                self.protocols[i].compute_roc()
            elif metric.upper() == 'ACC':
                self.protocols[i].compute_accuracy()
            elif metric.upper() == 'PR':
                self.protocols[i].compute_pr()
            else:
                self.protocols[i].compute_eer()
                self.protocols[i].compute_auc_roc()
                self.protocols[i].compute_auc_prc()

    def compute_all_metrics(self):
        for i in range(len(self.protocols)):
            self.protocols[i].compute_roc()
            self.protocols[i].compute_accuracy()
            self.protocols[i].compute_pr()
            self.protocols[i].compute_eer()
            self.protocols[i].compute_auc_roc()
            self.protocols[i].compute_auc_prc()

    def get_best_accuracy(self):
        accs = np.array([max(protocol.curves['ACC']['Acc']) for protocol in self.protocols])
        return accs.mean(), accs.std()


def load_single_template(item):
    """
    load single template
    :param item: feature path, (occ path), label, fmode, flatten
    :return:
    """
    if len(item) == 4:
        signature = Signature(item[0], fmode=item[2], flatten=item[3])
        if len(signature.features.shape) == 1:
            signature.features = signature.features.reshape((1, -1))
        return signature.features, item[1]
    elif len(item) == 5:
        signature = Signature(item[0], item[1], fmode=item[3], flatten=item[4])
        if len(signature.features.shape) == 1:
            signature.features = signature.features.reshape((1, -1))
        return signature.features, signature.occ, item[2]


class IdentificationOpenSetDataset(IMDB):
    def __init__(self, **kwargs):
        super(IdentificationOpenSetDataset, self).__init__(**kwargs)
        self.folds = [{'gallery': [], 'probe': []} for _ in range(self.num_folds)]

    def parse_data_set(self):
        raise NotImplementedError

    def load_and_compute_similarities(self, metric='cosine', **kwargs):
        """
        In this function, it expect the all templates are generated and save in the directory: ft_dir
        :param metric:
        :param kwargs:
        :return:
        """

        for i, fold in enumerate(self.folds):
            gallery_labels, probe_labels = [], []
            gallery_embeds, probe_embeds = [], []

            if isinstance(fold['gallery'][0], File):  # image-based recognition
                gallery_embeds_path_label = [[os.path.join(self.ft_dir, file.im_path.replace(self.im_ext, self.ft_ext)),
                                              file.subject_id, self.fmode, self.flatten] for file in fold['gallery']]

                probe_embeds_path_label = [[os.path.join(self.ft_dir, file.im_path.replace(self.im_ext, self.ft_ext)),
                                            file.subject_id, self.fmode, self.flatten] for file in fold['probe']]

            else:  # Set-based classification
                gallery_embeds_path_label = [[os.path.join(self.ft_dir, str(t.template_id) + self.ft_ext),
                                              t.subject_id, self.fmode, self.flatten]
                                             for t in fold['gallery']]
                probe_embeds_path_label = [[os.path.join(self.ft_dir, str(t.template_id) + self.ft_ext),
                                            t.subject_id, self.fmode, self.flatten]
                                           for t in fold['probe']]

            # enroll the gallery
            with Pool() as p:
                content = list(tqdm(p.imap(load_single_template, gallery_embeds_path_label),
                                    total=len(gallery_embeds_path_label)))

            for embeds, label in content:
                gallery_embeds.append(embeds)
                gallery_labels.append(label)

            gallery_embeds, gallery_labels = np.array(gallery_embeds), np.array(gallery_labels)

            # enroll the probe
            with Pool() as p:
                content = list(tqdm(p.imap(load_single_template, probe_embeds_path_label),
                                    total=len(probe_embeds_path_label)))

            for embeds, label in content:
                probe_embeds.append(embeds)
                probe_labels.append(label)

            probe_embeds, probe_labels = np.array(probe_embeds), np.array(probe_labels)

            # compute the similarity
            gallery_embeds, probe_embeds = np.concatenate(gallery_embeds), np.concatenate(probe_embeds)
            similarity = 1 - compute_pairwise_distances(gallery_embeds, probe_embeds, metric=metric, n_jobs=-1)

            self.add_protocol(gallery_labels, probe_labels, similarity)

    def generate_and_compute_similarities(self, inference, data_transform, bs, ctx=(mx.gpu(0)), norm_embeds=False,
                                          metric='cosine', output_transform=None, template_transform=None,
                                          save_template=False, **kwargs):
        os.environ['MXNET_CUDNN_AUTOTUNE_DEFAULT'] = '0'

        if isinstance(ctx, tuple):
            ctx = list(ctx)

        p_bar = tqdm(total=self.num_iters)

        template_transform = 'mean' if template_transform is None else template_transform

        if isinstance(template_transform, str):
            template_transform = RegisterTemplateTransform[template_transform]

        for i, fold in enumerate(self.folds):
            gallery_labels, probe_labels = [], []
            gallery_embeds, probe_embeds = [], []
            if isinstance(fold['gallery'][0], File): # image-based recognition
                # enroll gallery
                gallery_path_label = [[file.im_path, file.subject_id] for file in fold['gallery']]
                total = len(gallery_path_label)
                dataset = ImagePathLabelDataset(self.db_dir, gallery_path_label, transform=data_transform)
                bs = min(bs, total)
                dataloader = DataLoader(dataset, bs, last_batch='keep', pin_memory=True)

                for im_batch, lb_batch in dataloader:
                    im_lst = split_and_load(im_batch, ctx)
                    lb_lst = split_and_load(lb_batch, ctx)

                    for im, lb in zip(im_lst, lb_lst):
                        embeds = inference(im)

                        if isinstance(embeds, tuple) or isinstance(embeds, list):
                            embeds = output_transform(embeds)

                        if norm_embeds:
                            embeds = mx.nd.L2Normalization(embeds, mode='instance')

                        embeds = embeds.asnumpy()

                        gallery_embeds.append(embeds)
                        gallery_labels.append(lb.asnumpy())

                        p_bar.update(lb.shape[0])

                # enroll probe
                probe_path_label = [[file.im_path, file.subject_id] for file in fold['probe']]
                total = len(probe_path_label)
                dataset = ImagePathLabelDataset(self.db_dir, probe_path_label, transform=data_transform)
                bs = min(bs, total)
                dataloader = DataLoader(dataset, bs, last_batch='keep', pin_memory=True)

                for im_batch, lb_batch in dataloader:
                    im_lst = split_and_load(im_batch, ctx)
                    lb_lst = split_and_load(lb_batch, ctx)

                    for im, lb in zip(im_lst, lb_lst):
                        embeds = inference(im)

                        if isinstance(embeds, tuple) or isinstance(embeds, list):
                            embeds = output_transform(embeds)

                        if norm_embeds:
                            embeds = mx.nd.L2Normalization(embeds, mode='instance')

                        embeds = embeds.asnumpy()

                        probe_embeds.append(embeds)
                        probe_labels.append(lb.asnumpy())

                        p_bar.update(lb.shape[0])

            else:   # Set-based classification
                # enroll the gallery
                for t in fold['gallery']:
                    if save_template:
                        if os.path.exists(os.path.join(self.save_dir, str(t.template_id) + self.ft_ext)):
                            continue

                    im_paths = [im_path for im_path in t.im_paths if os.path.exists(os.path.join(self.db_dir, im_path))]
                    dataset = ImagePathDataset(self.db_dir, im_paths, transform=data_transform)
                    dataloader = DataLoader(dataset, min(bs, len(im_paths)), last_batch='keep', pin_memory=False)

                    template_embeds = []
                    for im_batch in dataloader:
                        im_lst = split_and_load(im_batch, ctx)

                        for im in im_lst:
                            embeds = inference(im)

                            if isinstance(embeds, tuple) or isinstance(embeds, list):
                                embeds = output_transform(embeds)

                            if self.flatten:
                                embeds = mx.nd.flatten(embeds)

                            if norm_embeds:
                                embeds = mx.nd.L2Normalization(embeds, mode='instance')

                            embeds = embeds.asnumpy()

                            template_embeds.append(embeds)

                    gallery_labels.append(t.subject_id)

                    template_embeds = template_transform(template_embeds)
                    gallery_embeds.append(template_embeds)

                    if save_template:
                        assert t.template_id is not None, 'Template ID cannot be None'
                        t.load_signature(template_embeds)
                        t.save_signature(os.path.join(self.save_dir, str(t.template_id) + self.ft_ext))

                    p_bar.update(1)

                # enroll the probe
                for t in fold['probe']:
                    im_paths = [im_path for im_path in t.im_paths if os.path.exists(os.path.join(self.db_dir, im_path))]
                    dataset = ImagePathDataset(self.db_dir, im_paths, transform=data_transform)
                    dataloader = DataLoader(dataset, min(bs, len(im_paths)), last_batch='keep', pin_memory=False)

                    template_embeds = []
                    for im_batch in dataloader:
                        im_lst = split_and_load(im_batch, ctx)

                        for im in im_lst:
                            embeds = inference(im)

                            if isinstance(embeds, tuple) or isinstance(embeds, list):
                                embeds = output_transform(embeds)

                            if self.flatten:
                                embeds = mx.nd.flatten(embeds)

                            if norm_embeds:
                                embeds = mx.nd.L2Normalization(embeds, mode='instance')

                            embeds = embeds.asnumpy()

                            template_embeds.append(embeds)

                    probe_labels.append(t.subject_id)

                    template_embeds = template_transform(template_embeds)
                    probe_embeds.append(template_embeds)

                    if save_template:
                        assert t.template_id is not None, 'Template ID cannot be None'
                        t.load_signature(template_embeds)
                        t.save_signature(os.path.join(self.save_dir, str(t.template_id) + self.ft_ext))

                    p_bar.update(1)

            # compute the similarity
            gallery_embeds, probe_embeds = np.concatenate(gallery_embeds), np.concatenate(probe_embeds)
            gallery_labels, probe_labels = np.concatenate(gallery_labels), np.concatenate(probe_labels)

            similarity = 1 - compute_pairwise_distances(gallery_embeds, probe_embeds, metric=metric, n_jobs=-1)

            self.add_protocol(gallery_labels, probe_labels, similarity)

        p_bar.close()

    def add_protocol(self, gallery_labels, probe_labels, similarity):
        self.protocols.append(BiometricOpenSearchProtocol(gallery_labels, probe_labels, similarity))

    def compute_metric(self, metric='IET'):
        for i in range(len(self.protocols)):
            if metric.upper() == 'CMC':
                self.protocols[i].compute_cmc_curve()
            elif metric.upper() == 'IET':
                self.protocols[i].compute_iet_curve()
            else:
                raise NotImplementedError

    def compute_all_metrics(self):
        for i in range(len(self.protocols)):
            self.protocols[i].compute_cmc_curve()
            self.protocols[i].compute_iet_curve()


class IdentificationCloseSetDataset(IdentificationOpenSetDataset):
    def __init__(self, **kwargs):
        super(IdentificationCloseSetDataset, self).__init__(**kwargs)

    def parse_data_set(self):
        raise NotImplementedError

    def add_protocol(self, gallery_labels, probe_labels, similarity):
        self.protocols.append(BiometricCloseSearchProtocol(gallery_labels, probe_labels, similarity))

    def compute_metric(self, metric='CMC'):
        for i in range(len(self.protocols)):
            if metric.upper() == 'CMC':
                self.protocols[i].compute_cmc_curve()
            else:
                raise NotImplementedError

    def compute_all_metrics(self):
        for i in range(len(self.protocols)):
            self.protocols[i].compute_cmc_curve()
